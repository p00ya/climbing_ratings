# Functions for pre-processing ascents data.

# Copyright 2019 Dean Scarff
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

library(dplyr)
library(stringr)


# Converts an ascent type like "Hang dog" to a code like "dog".  The codes
# are all lower-case and contain no spaces.
NormalizeAscentType <- function(ascent_type) {
  ascent_type %>%
    str_to_lower() %>%
    str_remove_all(fixed(" ")) %>%
    recode(hangdog = "dog")
}

# Appends NAs to each element of x such that they all have at least len
# elements.
PadList <- function(x, len) {
  purrr::map_if(x, ~ length(.) < len, ~ c(., rep(NA, len - length(.))))
}

# Converts a list of grades as they appear in theCrag's API responses to an
# atomic vector of integers representing the Ewbank grade.  Each input grade is
# expected to be a list containing (id, grade, context).  Contexts other than
# AU will return NA.
ExtractEwbankGrade <- function(grades) {
  # Empty lists are dropped by rbind; replace them with the right shape of
  # list containing NA.
  grades <- PadList(grades, 3)

  dfg <- as.data.frame(do.call(rbind, grades))
  colnames(dfg) <- c("id", "grade", "context")
  # Ignore "NAs introduced by coercion" warnings.
  suppressWarnings(ifelse(dfg$context == "AU", as.integer(dfg$grade), NA))
}

# Reads a CSV export of a climber's logbook from theCrag.  The filename is
# assumed to be in the format generated by theCrag, i.e.
# "climbername-logbook-YYYY-MM-DD.csv".
# Returns a "raw ascents" data frame with columns "ascentId", "route",
# "climber", "tick", "grade" and "timestamp".
ReadLogbook <- function(filename) {
  m <- str_match(basename(filename), "([^-]+)-logbook-\\d{4}-\\d{2}-\\d{2}")
  climber <- str_to_lower(m[[1, 2]])
  df <- read.csv(
    filename,
    # R doesn't support 64-bit integers, so force IDs to be interpreted as characters.
    colClasses = c(Ascent.ID = "character", Route.ID = "character"),
    stringsAsFactors = FALSE
  )
  df %>%
    select(Ascent.ID, Ascent.Type, Route.ID, Route.Grade, Ascent.Date) %>%
    mutate(
      route = factor(Route.ID),
      climber = climber,
      tick = factor(NormalizeAscentType(Ascent.Type)),
      grade = as.integer(Route.Grade),
      timestamp = as.integer(
        as.POSIXct(Ascent.Date, format = "%FT%H:%M:%SZ", optional = TRUE)
      )
    ) %>%
    na.omit() %>%
    select(ascentId = Ascent.ID, route, climber, tick, grade, timestamp)
}

# Reads all logbooks in directory "dir".  Logbook filenames are assumed to be
# in the format generated by theCrag, i.e. "climbername-logbook-YYYY-MM-DD.csv".
# Returns a "raw ascents" data frame with columns "ascentId", "route",
# "climber", "tick", "grade" and "timestamp".
ReadLogbooks <- function(dir) {
  logbooks <- Sys.glob(file.path(dir, "*-logbook-*.csv"))
  dfs <- purrr::map(logbooks, ReadLogbook)
  bind_rows(dfs)
}

# Downloads ascent data from theCrag's ascent facet API.  The data is paginated
# and saved as JSON responses in "dir".
FetchJsonFromApi <- function(area, api_key, dir, per_page = 5000) {
  base_url <- paste0(
    "https://sandpit.thecrag.com/api",
    "/facet/ascents/at/", area,
    "/with-route-gear-style/sport",
    "/in-setting/natural",
    "?thin=1",
    "&withdata=AscentID,Date,AccountID,NodeID,Tick,Grade",
    "&sortby=when",
    "&markupType=text",
    "&key=", api_key,
    "&perPage=", per_page
  )

  for (page in 1:1024) {
    url <- paste0(base_url, "&page=", page)
    filename <- file.path(dir, sprintf("ascents-%03d.json", page))

    if (download.file(url, filename, method = "libcurl")) {
      # Stop making requests if there was an error.
      warning(paste("Error downloading URL: ", url))
      break
    }

    j <- jsonlite::read_json(filename)
    # Note the perPage parameter of the request may not be respected; calculate
    # the actual pagination from the response.  The response may code these
    # fields as strings.
    j_page <- as.integer(j$data$page)
    j_num_ascents <- as.integer(j$data$numberAscents)
    j_per_page <- as.integer(j$data$perPage)
    # Stop on the last page.
    if (j_page * j_per_page >= j_num_ascents) {
      break
    }

    # Don't hammer theCrag's servers.
    Sys.sleep(1)
    page <- page + 1
  }
}

# The minimum set of fields required for parsing ascents from theCrag's ascent
# facet API.
kDefaultWithdata <- c(
  "AscentID", "Date", "AccountID", "NodeID", "Tick", "Grade"
)

# Parses ascent data from JSON responses as returned by theCrag's ascent facet
# API.  "withdata" is a character vector of the fields passed to the API's
# parameter of the same name.  It should be a superset of kDefaultWithdata.
# "json" should be a parsed JSON object as returned by jsonlite.
# Returns a "raw ascents" data frame with columns "ascentId", "route",
# "climber", "tick", "grade" and "timestamp".
ParseJsonAscents <- function(json, withdata = kDefaultWithdata) {
  # Due to the input JSON's use of heterogeneous arrays, what comes out of
  # jsonlite is horribly structured: ascents is a list, each ascent is a list,
  # ascents can have different lengths, and the fields in each ascent have no
  # names.
  ascents <- PadList(json$data$ascents, length(withdata))
  df_json <- as.data.frame(do.call(rbind, ascents))

  # Columns correspond to the "withdata" parameter of the API request.
  colnames(df_json) <- withdata

  df_json %>%
    transmute(
      # Don't coerce the IDs to factors; assume this will be done in
      # ReadAllJsonAscents after rbind'ing.
      ascentId = as.character(AscentID),
      route = as.character(NodeID),
      climber = as.character(AccountID),
      tick = NormalizeAscentType(Tick),
      grade = ExtractEwbankGrade(Grade),
      timestamp = suppressWarnings(as.integer(
        as.POSIXct(as.character(Date), format = "%FT%H:%M:%SZ", optional = TRUE)
      ))
    ) %>%
    na.omit()
}

# Reads all ascent JSON in directory "dir".  JSON filenames are assumed to have
# the pattern "ascents-*.json".  "withdata" is a character vector of the fields
# passed to the API's parameter of the same name.
# Returns a "raw ascents" data frame with columns "ascentId", "route",
# "climber", "tick", "grade" and "timestamp".
ReadAllJsonAscents <- function(dir, withdata = kDefaultWithdata) {
  responses <- Sys.glob(file.path(dir, "ascents-*.json"))
  purrr::map(responses, jsonlite::read_json) %>%
    purrr::map(ParseJsonAscents, withdata = withdata) %>%
    bind_rows() %>%
    mutate(
      route = factor(route),
      climber = factor(climber),
      tick = factor(tick)
    )
}

# Classifies tick types like "dog" into a logical indicating whether the
# ascent was clean, or NA.
# Note that we are optimistic about tick types that are ambiguous in practice,
# e.g. a plain "tick" is clean.
# Also note that no tick shift is applied - a clean top rope ascent is
# equivalent to an onsight.
IsTickClean <- function(ticktype) {
  # See https://www.thecrag.com/en/article/ticktypes
  case_when(
    ticktype %in% c(
      "onsight", "flash", "redpoint", "groundupredpoint",
      "pinkpoint", "clean", "onsightsolo", "topropeonsight", "topropeflash",
      "topropeclean", "secondclean", "leadsolo", "firstfreeascent",
      "tick", "second"
    ) ~ TRUE,
    ticktype %in% c(
      "dog", "attempt", "retreat", "working",
      "allfreewithrest", "toproperest", "ghost", "secondrest"
    ) ~ FALSE
  )
}

# Tidies a raw ascents table.
#
# Removes unclassifiable ascents, removes routes with less than 2 ascents, adds
# a "clean" column, and re-orders the route levels so the first route has the
# most ascents at the most common grade.
#
# Also prints a summary of the resulting data.
#
# df_raw is expected to have the columns "ascentId", "route", "climber",
# "tick", "grade", "timestamp".
CleanAscents <- function(df_raw) {
  df <- df_raw %>%
    mutate(clean = IsTickClean(tick)) %>%
    filter(!is.na(clean)) %>%
    filter(!is.na(grade))

  # Summarise routes by their grade and number of ascents:
  routes <- df %>%
    group_by(route) %>%
    summarise(n = n(), grade = floor(median(grade)))

  # Make the route with the most ascents for the most common grade the first
  # route.  This means it will be used as the reference route (natural rating
  # prior with mode 0).  Having the most common grade and lots of ascents means
  # it is (hopefully) a good reference point.
  route_grades <- routes %>% count(grade)
  routes <- routes %>%
    inner_join(route_grades, by = "grade", suffix = c(".ascents", ".grade")) %>%
    arrange(desc(n.grade), desc(n.ascents)) %>%
    select(-n.grade, -grade)

  # Drop ascents where the route has a single ascent.
  df <- df %>%
    inner_join(routes, by = "route") %>%
    filter(n.ascents > 1) %>%
    select(-n.ascents)

  # Find how often climbers log clean ascents:
  climbers <- df %>%
    group_by(climber) %>%
    summarise(clean_p = mean(clean))

  # Drop ascents where the climber hasn't logged any non-clean ascents.
  df <- df %>%
    inner_join(filter(climbers, clean_p < 1), by = "climber") %>%
    select(-clean_p)

  # Recompute factors from the preprocessed data.
  df <- df %>%
    mutate(
      route = route %>%
        droplevels() %>%
        relevel(ref = as.character(routes[[1, 1]])),
      climber = climber %>% droplevels()
    )

  # Summarize the tick counts.
  cat(
    prettyNum(nrow(df), big.mark = ","), "ascents by",
    prettyNum(nlevels(df$climber), big.mark = ","), "climbers, over",
    prettyNum(nlevels(df$route), big.mark = ","), "routes;",
    sprintf("%0.2f%%", mean(df$clean) * 100.0), "clean ascents\n"
  )

  df
}

# Given a data frame containing filtered ascents (as produced by
# "CleanAscents"), normalizes these to ascent, page and route tables.
#
# period_length is the number of seconds per page.
NormalizeTables <- function(df, period_length) {
  df_routes <- df %>%
    group_by(route) %>%
    summarise(ewbank = floor(median(grade)))

  df_ascents <- df %>%
    mutate(
      t = (timestamp - min(df$timestamp)) %/% period_length,
      clean = as.numeric(clean)
    ) %>%
    arrange(climber, t)

  df_pages <- df_ascents %>%
    group_by(climber, t) %>%
    summarise(timestamp = first(timestamp))

  # Set first_page[c] to be the index in df_pages of the first page for climber
  # c.
  first_page <- (df_pages %>% group_by(climber) %>% summarise(n = n()) %>%
    mutate(idx = head(cumsum(c(1, n)), -1)))$idx

  # time relative to the first ascent from the same climber
  df_pages$rel_t <- df_pages$t - df_pages$t[first_page[df_pages$climber]]
  # time relative to the next page for the same climber (meaningless for last
  # page of each climber).
  df_pages$gap <- c(diff(df_pages$rel_t), 0)

  df_pages <- df_pages %>%
    ungroup() %>%
    mutate(page = row_number()) %>%
    select(climber, t, gap, page, timestamp)

  df_ascents <- df_ascents %>%
    inner_join(df_pages, by = c("climber", "t")) %>%
    select(route, climber, clean, page)

  list(ascents = df_ascents, pages = df_pages, routes = df_routes)
}

# Performs a transformation of a grade to a "gamma" rating.
#
# The first rating is normalized to 1 (a natural rating of 0).
#
# This transformation assumes a linear relationship between the grade and the
# "natural" rating.  A scale of 1 implies that if a climber has a 50%
# probability of ascending a route at grade X cleanly, then they have a 1/(1+e)
# (approx. 27%) probability of ascending a route at grade X + 1 cleanly.
TransformGrade <- function(grade, scale = 0) {
  exp(scale * (grade - grade[[1]]))
}

# Writes normalized tables to CSV files.
#
# The files are written with standard names to the directory specified by
# "dir".
#
# "dfs" should be a list of data frames, with the tags "ascents", "routes" and
# "pages"; like what NormalizeTables returns.
WriteNormalizedTables <- function(dfs, dir) {
  write.csv(
    dfs$ascents %>%
      mutate(route = as.integer(route) - 1, page = page - 1) %>%
      select(-climber),
    file.path(dir, "ascents.csv"),
    row.names = FALSE
  )
  write.csv(
    dfs$routes %>% select(route, gamma),
    file.path(dir, "routes.csv"),
    row.names = FALSE
  )
  write.csv(
    dfs$pages %>%
      select(climber, gap) %>%
      mutate(climber = as.integer(climber) - 1),
    file.path(dir, "pages.csv"),
    row.names = FALSE
  )
}
